{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTzgIR5H-my",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation example\n",
        "\n",
        "- dataset: MNIST\n",
        "- layers: fullyconnected\n",
        "- activation: sigmoid\n",
        "- loss: categorical cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU9KnKQxH42P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzmH41xQIIO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FullyConnectedLayer():\n",
        "  \"\"\"\n",
        "    Fully Connected Layer\n",
        "    \n",
        "  \"\"\"\n",
        "  def __init__(self, n_outputs, seed=None,\n",
        "               initializer=np.random.standard_normal):\n",
        "    self.n_inputs = None\n",
        "    self.n_outputs = n_outputs\n",
        "    \n",
        "    # variables\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "    \n",
        "    # initializer\n",
        "    self._is_initialized = False\n",
        "    self.seed = seed\n",
        "    self.initializer = initializer\n",
        "    \n",
        "    # keep track of last output\n",
        "    self.outputs = {\n",
        "        \"z\": None\n",
        "    }\n",
        "  \n",
        "  def _initialize(self):\n",
        "    \"\"\"\n",
        "      Weights and bias initialization\n",
        "    \n",
        "    \"\"\"\n",
        "    np.random.seed(self.seed)\n",
        "    self.weights = self.initializer([self.n_inputs, self.n_outputs])\n",
        "    self.bias = self.initializer([1, self.n_outputs])\n",
        "    \n",
        "  def forward_pass(self, X):\n",
        "    \"\"\"\n",
        "      Evaluate output of forward pass\n",
        "      Lazy initialization on weights and bias\n",
        "      \n",
        "      Parameters\n",
        "      ----------\n",
        "      X: numpy array [batch_size, features]\n",
        "      \n",
        "      Output\n",
        "      ------\n",
        "      numpy array X * W + b\n",
        "    \n",
        "    \"\"\"\n",
        "    if self._is_initialized is False:\n",
        "      self.n_inputs = X.shape[1]\n",
        "      self._initialize()\n",
        "    \n",
        "    z = np.dot(X, self.weights) + self.bias  \n",
        "    self.outputs['z'] = np.copy(z)\n",
        "    \n",
        "    return z\n",
        "  \n",
        "  def backward_pass(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mECqobgwOSDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SigmoidLayer():\n",
        "  \n",
        "  def __init__(self):\n",
        "    # keep track of last output\n",
        "    self.outputs = {\n",
        "        \"z\": None\n",
        "    }\n",
        "  \n",
        "  def forward_pass(self, X):\n",
        "    \"\"\"\n",
        "      Apply pointwise sigmoid to inputs\n",
        "      \n",
        "      Parameters\n",
        "      ----------\n",
        "      X: numpy array [batch_size, ...]\n",
        "      \n",
        "      Output\n",
        "      ------\n",
        "      numpy array dimension: input dimension \n",
        "    \n",
        "    \"\"\"\n",
        "    z = 1.0 / (1.0 + np.exp(-X))\n",
        "    \n",
        "    self.outputs['z'] = z\n",
        "    \n",
        "    return z\n",
        "    \n",
        "  def backward_pass(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fog_NvAUSM5F",
        "colab_type": "text"
      },
      "source": [
        "## Softmax\n",
        "Practical implementation, to overcome numerical instabilities. \n",
        ">$\\mbox{softmax} : \\mathcal{R}^{(n, m)}\\rightarrow \\mathcal{R}^{(n, m)}$\n",
        "\n",
        ">$[\\mbox{softmax}(X)]_{i,j} = \\frac{\n",
        " e^{X_{i,j}}\n",
        " }\n",
        " {\n",
        " \\sum_j e^{X_{i,j}}\n",
        " } = \n",
        " \\frac{\n",
        " C e^{X_{i,j}}\n",
        " }\n",
        " {\n",
        " C \\sum_j e^{X_{i,j}}\n",
        " }\n",
        " =\n",
        " \\frac{\n",
        " e^{X_{i,j} + \\log C_i }\n",
        " }\n",
        " {\n",
        " \\sum_j e^{X_{i,j}+ \\log C_i}\n",
        " }\n",
        " $\n",
        " \n",
        "with \n",
        ">$\\log C_i = \\max_j X_{i,j}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Md2zwVOX9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "  \"\"\"\n",
        "    Evaluate softmax along axis=1\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X: numpy array [batch_size, classes]\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "    softmax along axis=1\n",
        "  \"\"\"\n",
        "  num = np.exp( logits - np.expand_dims(np.max(logits,axis=1), axis=1) )\n",
        "  z = num / np.expand_dims(np.sum(num, axis=1), axis=1)\n",
        "    \n",
        "  return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpK8XN_JYZgL",
        "colab_type": "text"
      },
      "source": [
        "## Cross Entropy\n",
        "Given $n$ batch size,  $c$ classes:\n",
        "> $p \\in \\mathcal{R}^{(n, c)} $: output of NN in terms of probability per class\n",
        "\n",
        "> $l \\in \\mathcal{R}^{(n, 1)} $: labels\n",
        "\n",
        "> $t \\in \\mathcal{R}^{(n, c)} $: onehot of labels\n",
        "\n",
        ">$ E( p; l ) = \\frac{1}{n} \\sum_{i} \\sum_j t_{i,j} \\log(p_{i,j}) = \\frac{1}{n} \\sum_i \\log(p_{i,l_{i}})$\n",
        "\n",
        "##Cross Entropy with logits\n",
        "\n",
        ">$ E( X; l ) = \\frac{1}{n} \\sum_i \\log(p_{i,l_{i}}) = \\frac{1}{n} \\sum_i \\log \\frac{\n",
        " e^{X_{i,l_i}}\n",
        " }\n",
        " {\n",
        " \\sum_j e^{X_{i,j}}\n",
        " }  $\n",
        " \n",
        " >$ = \\frac{1}{n} \\sum_i \\left( \\log(e^{X_{i,l_i}}) - \\log (\\sum_j e^{X_{i,j}}) \\right) = $\n",
        " \n",
        " > $ = \\frac{1}{n} \\sum_i \\left( X_{i,l_i} - \\log (\\sum_j e^{X_{i,j}}) \\right) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33y9Z74WYYhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(p, labels):\n",
        "  \"\"\"\n",
        "    Evaluate cross entropy\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    p: numpy array [batch_size, classes]\n",
        "    labels: numpy array [batch_size, 1]\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "    scalar\n",
        "  \"\"\"\n",
        "  batch_size = p.shape[0]\n",
        "      \n",
        "  log_likelihood = -np.log(p[range(batch_size), labels])\n",
        "  loss = np.mean(log_likelihood)\n",
        "  \n",
        "  return loss\n",
        "  \n",
        "  \n",
        "def cross_entropy_with_logits(logits, labels):\n",
        "  \"\"\"\n",
        "    Evaluate cross entropy\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    p: numpy array [batch_size, classes]\n",
        "    labels: numpy array [batch_size, 1]\n",
        "    \n",
        "    Output\n",
        "    ------\n",
        "    scalar\n",
        "  \"\"\"\n",
        "  loss = - np.mean(logits[range(batch_size), labels] - np.log(np.sum(np.exp(logits), axis=1)))\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocXEooaiSvbe",
        "colab_type": "code",
        "outputId": "bc23727d-95fc-41fb-d23a-9d94b0aa5dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " # Load dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape to have one row per observation\n",
        "X_train = X_train.reshape(-1, 28*28)\n",
        "X_test = X_test.reshape(-1, 28*28)\n",
        "\n",
        "# reshape to have one column labels\n",
        "y_train = np.expand_dims(y_train, axis=1)\n",
        "y_test = np.expand_dims(y_test, axis=1)  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIEb7Xi9eQ7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(net, batch):\n",
        "   # forward pass\n",
        "  output = batch\n",
        "  for layer in net:\n",
        "    output = layer.forward_pass(output)\n",
        "      \n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2nLfGBiWUU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = []\n",
        "\n",
        "net.append(FullyConnectedLayer(10, seed=42, initializer=(lambda x: 0.05*np.random.standard_normal(x)) ))\n",
        "# net.append(SigmoidLayer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs0pIKLsXiTL",
        "colab_type": "code",
        "outputId": "4714d015-8d6c-4774-e86e-7b543ed9a1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch_size = 1000\n",
        "batch_features = X_train[:batch_size]\n",
        "batch_labels = y_train[:batch_size]\n",
        "\n",
        "\n",
        "logits = forward_pass(net, batch_features)\n",
        "p = softmax(logits)\n",
        "loss = cross_entropy(p, batch_labels)\n",
        "\n",
        "loss2 = cross_entropy_with_logits(logits, batch_labels)\n",
        "\n",
        "print(loss - loss2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-7.284492653525376e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwgPlskFZQTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_batcher(features, labels, batch_size=100):\n",
        "    # Provide chunks one by one\n",
        "    start = 0\n",
        "    N = len(features)\n",
        "    while start < N:\n",
        "        rows = range(start,start+batch_size)\n",
        "        start += batch_size\n",
        "        yield features[rows], labels[rows,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHvVltDOcZk2",
        "colab_type": "code",
        "outputId": "bdf14755-d808-4f3b-ea12-994ba0d6f3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "source": [
        "n_epochs=2\n",
        "batch_size = 2000\n",
        "\n",
        "net = []\n",
        "net.append(FullyConnectedLayer(10, seed=42, initializer=(lambda x: 0.01*np.random.standard_normal(x)) ))\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for idx, (features, labels) in enumerate(epoch_batcher(X_train, y_train, batch_size=batch_size)):\n",
        "    \n",
        "    logits = forward_pass(net, features)\n",
        "    loss = cross_entropy_with_logits(logits, labels)\n",
        "\n",
        "    print('epoch {: 4d}, batch {: 4d} loss {:6.4e}'.format(epoch,idx,loss))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch    0, batch    0 loss 3.3368e+01\n",
            "epoch    0, batch    1 loss 3.4153e+01\n",
            "epoch    0, batch    2 loss 3.4138e+01\n",
            "epoch    0, batch    3 loss 3.3276e+01\n",
            "epoch    0, batch    4 loss 3.4029e+01\n",
            "epoch    0, batch    5 loss 3.4354e+01\n",
            "epoch    0, batch    6 loss 3.4320e+01\n",
            "epoch    0, batch    7 loss 3.3618e+01\n",
            "epoch    0, batch    8 loss 3.3855e+01\n",
            "epoch    0, batch    9 loss 3.4344e+01\n",
            "epoch    0, batch   10 loss 3.3890e+01\n",
            "epoch    0, batch   11 loss 3.4459e+01\n",
            "epoch    0, batch   12 loss 3.4357e+01\n",
            "epoch    0, batch   13 loss 3.3758e+01\n",
            "epoch    0, batch   14 loss 3.4335e+01\n",
            "epoch    0, batch   15 loss 3.3450e+01\n",
            "epoch    0, batch   16 loss 3.4032e+01\n",
            "epoch    0, batch   17 loss 3.3693e+01\n",
            "epoch    0, batch   18 loss 3.4438e+01\n",
            "epoch    0, batch   19 loss 3.3808e+01\n",
            "epoch    0, batch   20 loss 3.3207e+01\n",
            "epoch    0, batch   21 loss 3.3785e+01\n",
            "epoch    0, batch   22 loss 3.3605e+01\n",
            "epoch    0, batch   23 loss 3.3775e+01\n",
            "epoch    0, batch   24 loss 3.4288e+01\n",
            "epoch    0, batch   25 loss 3.4265e+01\n",
            "epoch    0, batch   26 loss 3.3759e+01\n",
            "epoch    0, batch   27 loss 3.5161e+01\n",
            "epoch    0, batch   28 loss 3.3475e+01\n",
            "epoch    0, batch   29 loss 3.3934e+01\n",
            "epoch    1, batch    0 loss 3.3368e+01\n",
            "epoch    1, batch    1 loss 3.4153e+01\n",
            "epoch    1, batch    2 loss 3.4138e+01\n",
            "epoch    1, batch    3 loss 3.3276e+01\n",
            "epoch    1, batch    4 loss 3.4029e+01\n",
            "epoch    1, batch    5 loss 3.4354e+01\n",
            "epoch    1, batch    6 loss 3.4320e+01\n",
            "epoch    1, batch    7 loss 3.3618e+01\n",
            "epoch    1, batch    8 loss 3.3855e+01\n",
            "epoch    1, batch    9 loss 3.4344e+01\n",
            "epoch    1, batch   10 loss 3.3890e+01\n",
            "epoch    1, batch   11 loss 3.4459e+01\n",
            "epoch    1, batch   12 loss 3.4357e+01\n",
            "epoch    1, batch   13 loss 3.3758e+01\n",
            "epoch    1, batch   14 loss 3.4335e+01\n",
            "epoch    1, batch   15 loss 3.3450e+01\n",
            "epoch    1, batch   16 loss 3.4032e+01\n",
            "epoch    1, batch   17 loss 3.3693e+01\n",
            "epoch    1, batch   18 loss 3.4438e+01\n",
            "epoch    1, batch   19 loss 3.3808e+01\n",
            "epoch    1, batch   20 loss 3.3207e+01\n",
            "epoch    1, batch   21 loss 3.3785e+01\n",
            "epoch    1, batch   22 loss 3.3605e+01\n",
            "epoch    1, batch   23 loss 3.3775e+01\n",
            "epoch    1, batch   24 loss 3.4288e+01\n",
            "epoch    1, batch   25 loss 3.4265e+01\n",
            "epoch    1, batch   26 loss 3.3759e+01\n",
            "epoch    1, batch   27 loss 3.5161e+01\n",
            "epoch    1, batch   28 loss 3.3475e+01\n",
            "epoch    1, batch   29 loss 3.3934e+01\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}